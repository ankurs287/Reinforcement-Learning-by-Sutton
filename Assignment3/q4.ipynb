{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIT = 'Hit'\n",
    "STICK = 'Stick'\n",
    "actions = [STICK, HIT]\n",
    "value = np.zeros((10, 10, 2))\n",
    "TENK = 10000\n",
    "FIVEL = 500000\n",
    "TRUE_VALUE = -0.27726\n",
    "\n",
    "class State:\n",
    "    def __init__(self, player_usable_ace, player_card_sum, dealer_face_up_card):\n",
    "        self.player_usable_ace = player_usable_ace\n",
    "        self.dealer_face_up_card = dealer_face_up_card\n",
    "        self.player_card_sum = player_card_sum\n",
    "\n",
    "\n",
    "class Env:\n",
    "    def __init__(self, state=None, action=None):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        if state is not None:\n",
    "            self.set_dealer_face_down_card()\n",
    "\n",
    "    @staticmethod\n",
    "    def equi_probable_random_state_action_pair():\n",
    "        env = Env()\n",
    "        player_card_sum = np.random.randint(12, 21 + 1)\n",
    "        dealer_face_up_card = np.random.randint(1, 10 + 1)\n",
    "        player_usable_ace = bool(np.random.randint(2))\n",
    "        env.state = State(player_usable_ace, player_card_sum, dealer_face_up_card)\n",
    "        env.action = np.random.choice(actions)\n",
    "        env.set_dealer_face_down_card()\n",
    "        return env\n",
    "\n",
    "    def set_dealer_face_down_card(self):\n",
    "        dealer_face_down_card = hit()\n",
    "        dealer_card_sum = 0\n",
    "        dealer_usable_ace = True\n",
    "        if is_ace(self.state.dealer_face_up_card) and is_ace(dealer_face_down_card):\n",
    "            dealer_card_sum = 12\n",
    "        elif is_ace(self.state.dealer_face_up_card):\n",
    "            dealer_card_sum = 11 + dealer_face_down_card\n",
    "        elif is_ace(dealer_face_down_card):\n",
    "            dealer_card_sum = 11 + self.state.dealer_face_up_card\n",
    "        else:\n",
    "            self.state.dealer_face_up_card + dealer_face_down_card\n",
    "            dealer_usable_ace = False\n",
    "        self.dealer_usable_ace = dealer_usable_ace\n",
    "        self.dealer_card_sum = dealer_card_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_face_card(card):\n",
    "    return True if card >= 11 else False\n",
    "\n",
    "\n",
    "def is_ace(card):\n",
    "    return True if card == 1 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draws a card from a standard 52-card deck with replacement and return card's value\n",
    "def hit():\n",
    "    # randomly draw a card from a suit (size = 13). 11, 12, 13 represents 3 Face Cards\n",
    "    card = np.random.randint(1, 13 + 1)\n",
    "    if is_face_card(card):\n",
    "        return 10\n",
    "    else:\n",
    "        return card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_policy(player_card_sum):\n",
    "    if player_card_sum >= 20:\n",
    "        return STICK\n",
    "    return HIT\n",
    "\n",
    "\n",
    "def behavior_policy(player_card_sum):\n",
    "    return np.random.choice(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_env():\n",
    "    # init player's card sum (12-21) [i.e, sum of all the cards player holds]\n",
    "    player_card_sum = 0\n",
    "    player_ace_count = 0\n",
    "    player_usable_ace = False\n",
    "\n",
    "    while player_card_sum < 12:\n",
    "        card = hit()\n",
    "\n",
    "        if is_ace(card):\n",
    "            player_ace_count += 1\n",
    "            if player_card_sum + 11 <= 21:\n",
    "                player_usable_ace = True\n",
    "                player_card_sum += 11\n",
    "            else:\n",
    "                player_card_sum += 1\n",
    "\n",
    "        else:\n",
    "            player_card_sum += card\n",
    "\n",
    "    # init dealer's state\n",
    "    dealer_face_up_card = hit()\n",
    "    \n",
    "    initial_state = State(player_usable_ace, player_card_sum, dealer_face_up_card)\n",
    "    return Env(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, policy=target_policy):\n",
    "    # Generate an episode following pi from initial state\n",
    "    episode = []\n",
    "\n",
    "    player_card_sum = env.state.player_card_sum\n",
    "    player_usable_ace = env.state.player_usable_ace\n",
    "    dealer_face_up_card = env.state.dealer_face_up_card\n",
    "    dealer_card_sum = env.dealer_card_sum\n",
    "    dealer_usable_ace = env.dealer_usable_ace\n",
    "\n",
    "    # Player Act until he sticks or goes bust\n",
    "    action = policy(player_card_sum) if env.action is None else env.action\n",
    "    while True:\n",
    "        state = State(player_usable_ace, player_card_sum, dealer_face_up_card)\n",
    "        episode.append([state, action])\n",
    "        if action == HIT:\n",
    "            card = hit()\n",
    "            player_card_sum += card\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        if player_card_sum > 21 and player_usable_ace:\n",
    "            player_card_sum -= 10\n",
    "            player_usable_ace = False\n",
    "        elif player_card_sum > 21:\n",
    "            return -1, episode\n",
    "        action = policy(player_card_sum)\n",
    "\n",
    "    # Dealer Act\n",
    "    while True:\n",
    "        if dealer_card_sum < 17:\n",
    "            card = hit()\n",
    "            if is_ace(card) and dealer_card_sum + 11 < 22:\n",
    "                dealer_card_sum += 11\n",
    "                dealer_usable_ace = True\n",
    "            else:\n",
    "                dealer_card_sum += card\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        if dealer_card_sum > 21 and dealer_usable_ace:\n",
    "            dealer_card_sum -= 10\n",
    "            dealer_usable_ace = False\n",
    "        elif dealer_card_sum > 21:\n",
    "            return 1, episode\n",
    "\n",
    "    if player_card_sum > dealer_card_sum:\n",
    "        return 1, episode\n",
    "    elif player_card_sum == dealer_card_sum:\n",
    "        return 0, episode\n",
    "    return -1, episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def every_visit_mc(n_episodes):\n",
    "    state_value = np.zeros((2, 10, 10))\n",
    "    count_usable = np.ones(state_value.shape)\n",
    "    for i in range(n_episodes):\n",
    "        # Choose initial state(player cards sum, dealer's face-up card, usable or non-usable ace) randomly\n",
    "        env = init_env()\n",
    "        g, episode = play(env)\n",
    "        T = len(episode)\n",
    "        for t in range(T - 1, -1, -1):\n",
    "            state, action = episode[t]\n",
    "            player_card_sum = state.player_card_sum - 12\n",
    "            dealer_face_up_card = state.dealer_face_up_card - 1\n",
    "            player_usable_ace = state.player_usable_ace\n",
    "            u = 1 if player_usable_ace else 0\n",
    "            state_value[u, player_card_sum, dealer_face_up_card] += g\n",
    "            count_usable[u, player_card_sum, dealer_face_up_card] += 1\n",
    "    \n",
    "    state_value = state_value / count_usable\n",
    "    return state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_value_tk = every_visit_mc(TENK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_value_fl = every_visit_mc(FIVEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usable Ace, Episodes:  10000\n",
      "[[ 0.   0.4  0.  -0.4  0.1  0.   0.  -0.3 -0.4  0.2]\n",
      " [-0.3 -0.2 -0.5 -0.2 -0.5  0.  -0.4 -0.6 -0.4 -0.5]\n",
      " [-0.4 -0.7 -0.2 -0.2 -0.4 -0.6  0.  -0.3 -0.2 -0.2]\n",
      " [-0.4 -0.3  0.  -0.4 -0.1 -0.7 -0.3 -0.7 -0.5 -0.4]\n",
      " [-0.1 -0.2 -0.4 -0.5 -0.2 -0.3 -0.5  0.2 -0.5 -0.5]\n",
      " [-0.6 -0.1 -0.6 -0.1 -0.1 -0.4 -0.5 -0.4 -0.6 -0.5]\n",
      " [-0.6 -0.6 -0.4 -0.1 -0.3 -0.6 -0.6 -0.6 -0.2 -0.6]\n",
      " [-0.4 -0.3 -0.6 -0.4 -0.3 -0.5 -0.6 -0.7 -0.7 -0.6]\n",
      " [ 0.4  0.4  0.7  0.6  0.7  0.8  0.5  0.7  0.4  0.4]\n",
      " [ 0.6  0.8  0.9  0.8  0.9  0.8  0.9  0.8  0.8  0.9]]\n",
      "--------------------\n",
      "No Usable Ace, Episodes:  10000\n",
      "[[-0.7 -0.5 -0.6 -0.5 -0.6 -0.5 -0.6 -0.5 -0.5 -0.6]\n",
      " [-0.5 -0.6 -0.6 -0.6 -0.6 -0.4 -0.6 -0.6 -0.7 -0.6]\n",
      " [-0.6 -0.6 -0.5 -0.6 -0.7 -0.6 -0.7 -0.7 -0.6 -0.7]\n",
      " [-0.6 -0.6 -0.7 -0.6 -0.7 -0.6 -0.6 -0.7 -0.6 -0.7]\n",
      " [-0.7 -0.6 -0.5 -0.7 -0.7 -0.7 -0.6 -0.6 -0.6 -0.7]\n",
      " [-0.7 -0.7 -0.8 -0.7 -0.7 -0.5 -0.7 -0.8 -0.7 -0.7]\n",
      " [-0.8 -0.6 -0.6 -0.6 -0.7 -0.7 -0.6 -0.7 -0.7 -0.7]\n",
      " [-0.8 -0.7 -0.7 -0.7 -0.8 -0.7 -0.7 -0.7 -0.7 -0.8]\n",
      " [ 0.2  0.6  0.6  0.7  0.5  0.6  0.6  0.6  0.5  0.4]\n",
      " [ 0.6  0.9  0.9  0.9  0.9  0.9  0.8  0.9  0.9  0.8]]\n",
      "\n",
      "\n",
      "\n",
      "Usable Ace, Episodes:  500000\n",
      "[[-0.3 -0.3 -0.3 -0.3 -0.3 -0.2 -0.2 -0.3 -0.4 -0.3]\n",
      " [-0.4 -0.3 -0.3 -0.2 -0.3 -0.3 -0.3 -0.3 -0.2 -0.3]\n",
      " [-0.4 -0.3 -0.3 -0.4 -0.3 -0.3 -0.3 -0.3 -0.2 -0.3]\n",
      " [-0.5 -0.4 -0.4 -0.4 -0.3 -0.4 -0.4 -0.3 -0.4 -0.4]\n",
      " [-0.5 -0.4 -0.4 -0.4 -0.4 -0.4 -0.3 -0.4 -0.4 -0.4]\n",
      " [-0.5 -0.4 -0.4 -0.4 -0.4 -0.3 -0.4 -0.4 -0.4 -0.4]\n",
      " [-0.5 -0.4 -0.4 -0.3 -0.5 -0.5 -0.5 -0.5 -0.4 -0.5]\n",
      " [-0.6 -0.4 -0.5 -0.5 -0.5 -0.5 -0.4 -0.5 -0.4 -0.5]\n",
      " [ 0.1  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.5  0.4]\n",
      " [ 0.6  0.9  0.9  0.9  0.9  0.9  0.9  0.9  0.9  0.8]]\n",
      "--------------------\n",
      "No Usable Ace, Episodes:  500000\n",
      "[[-0.6 -0.5 -0.6 -0.5 -0.6 -0.6 -0.6 -0.5 -0.6 -0.6]\n",
      " [-0.7 -0.6 -0.6 -0.6 -0.6 -0.6 -0.6 -0.6 -0.6 -0.6]\n",
      " [-0.7 -0.6 -0.6 -0.6 -0.6 -0.6 -0.6 -0.6 -0.6 -0.6]\n",
      " [-0.7 -0.6 -0.6 -0.6 -0.6 -0.6 -0.6 -0.6 -0.6 -0.7]\n",
      " [-0.7 -0.7 -0.7 -0.7 -0.7 -0.7 -0.7 -0.7 -0.7 -0.7]\n",
      " [-0.8 -0.7 -0.7 -0.7 -0.7 -0.7 -0.7 -0.7 -0.7 -0.7]\n",
      " [-0.8 -0.7 -0.7 -0.7 -0.7 -0.7 -0.7 -0.7 -0.7 -0.7]\n",
      " [-0.8 -0.7 -0.7 -0.7 -0.7 -0.7 -0.7 -0.7 -0.7 -0.7]\n",
      " [ 0.1  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.5  0.5]\n",
      " [ 0.6  0.9  0.9  0.9  0.9  0.9  0.9  0.9  0.9  0.8]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Usable Ace, Episodes: \", TENK)\n",
    "print(np.round(state_value_tk[1], 1))\n",
    "print('-' * 20)\n",
    "print(\"No Usable Ace, Episodes: \", TENK)\n",
    "print(np.round(state_value_tk[0], 1))\n",
    "print(\"\\n\" * 2)\n",
    "\n",
    "print(\"Usable Ace, Episodes: \", FIVEL)\n",
    "print(np.round(state_value_fl[1], 1))\n",
    "print('-' * 20)\n",
    "print(\"No Usable Ace, Episodes: \", FIVEL)\n",
    "print(np.round(state_value_fl[0], 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Exploring Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_mc():\n",
    "    state_action_value = np.zeros((2, 10, 10, 2))  # usable ace x player card sum x dealer's face up card x actions\n",
    "    count = np.ones(state_action_value.shape)\n",
    "    EPISODES = FIVEL\n",
    "    for i in range(EPISODES):\n",
    "        # Choose initial state(player cards sum, dealer's face-up card, usable or non-usable ace) randomly\n",
    "        env = Env.equi_probable_random_state_action_pair()\n",
    "        g, episode = play(env)\n",
    "        T = len(episode)\n",
    "        for t in range(T - 1, -1, -1):\n",
    "            state, action = episode[t]\n",
    "            player_card_sum = state.player_card_sum - 12\n",
    "            dealer_face_up_card = state.dealer_face_up_card - 1\n",
    "            player_usable_ace = state.player_usable_ace\n",
    "            u = 1 if player_usable_ace else 0\n",
    "            state_action_value[u, player_card_sum, dealer_face_up_card, actions.index(action)] += g\n",
    "            count[u, player_card_sum, dealer_face_up_card, actions.index(action)] += 1\n",
    "\n",
    "    state_action_value = state_action_value / count\n",
    "    return state_action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_action_value = es_mc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usable Ace Optimal Value Function\n",
      "[[-0.4 -0.2 -0.2 -0.3 -0.3 -0.2 -0.2 -0.2 -0.3 -0.3]\n",
      " [-0.5 -0.3 -0.4 -0.3 -0.3 -0.2 -0.3 -0.3 -0.2 -0.3]\n",
      " [-0.4 -0.3 -0.4 -0.3 -0.3 -0.3 -0.3 -0.3 -0.3 -0.3]\n",
      " [-0.5 -0.4 -0.4 -0.3 -0.3 -0.4 -0.3 -0.4 -0.4 -0.4]\n",
      " [-0.5 -0.4 -0.4 -0.3 -0.4 -0.4 -0.4 -0.3 -0.4 -0.4]\n",
      " [-0.5 -0.3 -0.3 -0.3 -0.3 -0.2 -0.4 -0.3 -0.4 -0.3]\n",
      " [-0.3 -0.  -0.  -0.   0.   0.1 -0.  -0.1 -0.1 -0. ]\n",
      " [-0.1  0.3  0.3  0.2  0.3  0.3  0.3  0.2  0.2  0.2]\n",
      " [ 0.2  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.5  0.5]\n",
      " [ 0.6  0.9  0.9  0.9  0.9  0.9  0.9  0.9  0.9  0.8]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "No Usable Ace Optimal Value Function\n",
      "[[-0.6 -0.5 -0.5 -0.4 -0.4 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.7 -0.5 -0.5 -0.4 -0.4 -0.5 -0.4 -0.5 -0.4 -0.5]\n",
      " [-0.7 -0.4 -0.5 -0.4 -0.4 -0.5 -0.5 -0.5 -0.5 -0.4]\n",
      " [-0.7 -0.4 -0.4 -0.4 -0.4 -0.5 -0.4 -0.5 -0.5 -0.5]\n",
      " [-0.7 -0.4 -0.4 -0.4 -0.4 -0.5 -0.5 -0.5 -0.4 -0.5]\n",
      " [-0.6 -0.3 -0.3 -0.3 -0.3 -0.3 -0.3 -0.3 -0.4 -0.3]\n",
      " [-0.4 -0.   0.  -0.   0.   0.1  0.  -0.1 -0.1 -0.1]\n",
      " [-0.1  0.3  0.2  0.2  0.3  0.3  0.3  0.2  0.2  0.1]\n",
      " [ 0.2  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.5  0.5]\n",
      " [ 0.6  0.9  0.9  0.9  0.9  0.9  0.9  0.9  0.9  0.8]]\n",
      "\n",
      "\n",
      "\n",
      "1: HIT, 0: STICK\n",
      "Usable Ace Optimal Policy\n",
      "[[1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1]\n",
      " [1 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "No Usable Ace Optimal Policy\n",
      "[[1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n",
      "1: HIT, 0: STICK\n"
     ]
    }
   ],
   "source": [
    "print('Usable Ace Optimal Value Function')\n",
    "print(np.round(np.max(state_action_value[1], axis=-1), 1))\n",
    "print('-' * 100)\n",
    "print('No Usable Ace Optimal Value Function')\n",
    "print(np.round(np.max(state_action_value[0], axis=-1), 1))\n",
    "print('\\n' * 2)\n",
    "\n",
    "print('1: HIT, 0: STICK')\n",
    "print('Usable Ace Optimal Policy')\n",
    "print(np.argmax(state_action_value[1], axis=-1))\n",
    "print('-' * 100)\n",
    "print('No Usable Ace Optimal Policy')\n",
    "print(np.argmax(state_action_value[0], axis=-1))\n",
    "print('1: HIT, 0: STICK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off-Policy MC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_policy_mc(n_episodes):\n",
    "    rhos = np.zeros(n_episodes)\n",
    "    returns = np.zeros(n_episodes)\n",
    "    for i in range(n_episodes):\n",
    "        # Choose initial state(player cards sum, dealer's face-up card, usable or non-usable ace) randomly\n",
    "        state = State(player_usable_ace=True, player_card_sum=13, dealer_face_up_card=2)\n",
    "        env = Env(state=state)\n",
    "        g, episode = play(env, behavior_policy)\n",
    "        T = len(episode)\n",
    "        rho = 1.0\n",
    "        for t in range(T - 1, -1, -1):\n",
    "            state, action = episode[t]\n",
    "            if action == target_policy(state.player_card_sum):\n",
    "                rho *= 2\n",
    "            else:\n",
    "                rho = 0.0\n",
    "                break\n",
    "        rhos[i] = rho\n",
    "        returns[i] = g\n",
    "    return np.add.accumulate(rhos * returns), np.add.accumulate(rhos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:38<00:00,  2.58it/s]\n"
     ]
    }
   ],
   "source": [
    "episodes = TENK\n",
    "runs = 100\n",
    "error_ordinary = np.zeros((runs, episodes))\n",
    "error_weighted = np.zeros((runs, episodes))\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(0, runs)):\n",
    "    weighted_returns_sum, rhos_sum = off_policy_mc(episodes)\n",
    "    ordinary_sampling = weighted_returns_sum / np.arange(1, episodes + 1)\n",
    "\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        weighted_sampling = np.where(rhos_sum != 0, weighted_returns_sum / rhos_sum, 0)\n",
    "\n",
    "    error_ordinary[i] = np.power(ordinary_sampling - TRUE_VALUE, 2)\n",
    "    error_weighted[i] = np.power(weighted_sampling - TRUE_VALUE, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.mean(error_ordinary, axis=0), label='Ordinary Importance Sampling')\n",
    "plt.plot(np.mean(error_weighted, axis=0), label='Weighted Importance Sampling')\n",
    "plt.xlabel('Episodes (log scale)')\n",
    "plt.ylabel('Mean square error')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.savefig('q4_3.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
